# RAG

> Configure your end-to-end RAG

## RAG Customization

RAG (Retrieval-Augmented Generation) in R2R can be extensively customized to suit various use cases. The main components for customization are:

1. **Generation Configuration**: Control the language model's behavior.
2. **Search Settings**: Fine-tune the retrieval process.
3. **Task Prompt Override**: Customize the system prompt for specific tasks.

### LLM Provider Configuration

Refer to the LLM configuration [page here](/self-hosting/configuration/llm).

### Retrieval Configuration

Refer to the retrieval configuration [page here](/self-hosting/configuration/retrieval/overview).

### Combining LLM and Retrieval Configuration for RAG

The `rag_generation_config` parameter allows you to customize the language model's behavior. Default settings are set on the server-side using the `r2r.toml`, as described in in previous configuraiton guides. These settings can be overridden at runtime as shown below:

```python
# Configure graphRAG search
# Configure LLM generation
rag_generation_config = {
    "model": "anthropic/claude-3-opus-20240229",
    "temperature": 0.7,
    "top_p": 0.95,
    "max_tokens_to_sample": 1500,
    "stream": True,
    "functions": None,  # For function calling, if supported
    "tools": None,  # For tool use, if supported
    "add_generation_kwargs": {},  # Additional provider-specific parameters
    "api_base": None  # Custom API endpoint, if needed
}
```

When performing a RAG query you can combine these vector search, knowledge graph search, and generation settings at runtime:

```python
response = client.retrieval.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config,
    search_settings={
        "use_semantic_search": True,
        "limit": 20,
        "use_hybrid_search": True,
    }
)
```

R2R defaults to the specified server-side settings when no runtime overrides are specified.

### RAG Prompt Override

For specialized tasks, you can override the default RAG task prompt at runtime:

```python
task_prompt_override = """You are an AI assistant specializing in quantum computing.
Your task is to provide a concise summary of the latest advancements in the field,
focusing on practical applications and breakthroughs from the past year."""

response = client.retrieval.rag(
    "What are the latest advancements in quantum computing?",
    rag_generation_config=rag_generation_config,
    task_prompt_override=task_prompt_override
)
```

This prompt can also be set statically on as part of the server configuration process.

## Agent-based Interaction

R2R supports multi-turn conversations and complex query processing through its agent endpoint:

```python
messages = [
    {"role": "system", "content": "You are a helpful AI assistant."},
    {"role": "user", "content": "What are the key differences between quantum and classical computing?"}
]

response = client.retrieval.agent(
    messages=messages,
    vector_search_settings=vector_search_settings,
    rag_generation_config=rag_generation_config,
)
```

The agent can break down complex queries into sub-tasks, leveraging both retrieval and generation capabilities to provide comprehensive responses. The settings specified in the example above will propagate to the agent and it's tools.

By leveraging these configuration options, you can fine-tune R2R's retrieval and generation process to best suit your specific use case and requirements.
